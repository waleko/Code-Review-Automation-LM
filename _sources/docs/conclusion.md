# Conclusion
In our exploration of code review data collection and model inference, we have gained valuable insights into the capabilities and limitations of language models in the context of code review. This journey has encompassed various notebooks, each focusing on a specific aspect of the process. Here, we summarize our key findings and the implications of our work:

- Language models show promise in generating code reviews, but there is ample room for improvement in terms of review quality, context, and relevance.

- Fine-tuning models on code review datasets is a valuable approach to enhance their performance, but further research is needed to optimize fine-tuning techniques.

- While models can assist in code reviews, they should be viewed as complementary tools to human reviewers rather than replacements. Human expertise remains invaluable in the code review process.

- Future work may involve exploring more advanced language models, experimenting with different fine-tuning strategies, and incorporating user feedback to refine predictions.

In conclusion, our journey through code review data collection and model inference has provided valuable insights into the potential of language models in code review automation. While challenges remain, these models have the potential to augment the code review process, helping developers produce higher-quality code. As technology continues to advance, we anticipate exciting developments in this field and a continued focus on improving the effectiveness of code review automation.

## Bibliography

```{bibliography}
```
