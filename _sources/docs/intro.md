# Code Review Automation with Language Models

## Introduction

In this series of Jupyter notebooks, we embark on a journey to collect code review data from GitHub repositories and
perform code review predictions using language models. Our primary goal is to explore the capabilities of different
models in generating code reviews and evaluate their performance.

### Collecting Code Review Data

In this initial notebook, we dive into the process of collecting code review data from GitHub repositories. We leverage
the PyGithub library to interact with the GitHub API, ensuring seamless data retrieval.

We establish a function to collect code review data from a GitHub repository, allowing us to specify parameters such as
the number of comments to load, skipping author comments, and more. The collected data is structured into a Pandas
DataFrame for further analysis and processing.

Three prominent repositories, namely `microsoft/vscode`, `JetBrains/kotlin`, and `transloadit/uppy`, are selected for
data collection due to their popularity and rich code review history. Additionally, we are going to use data from the
original CodeReviewer dataset `msg-test` that is provided by the authors of {cite}`li2022codereviewer`.

### CodeReviewer Model Inference

The second notebook focuses on generating code reviews using the `microsoft/codereviewer` model. We delve into the
tokenization and dataset preparation process, emphasizing the importance of specialized tokens.

A custom `ReviewsDataset` class is introduced to facilitate data loading and transformation, making it compatible with
model inference. We load data from various sources, creating DataLoader instances for efficient model input.

We explore the model inference process, employing both a HuggingFace pre-trained checkpoint and a fine-tuned
CodeReviewer model. The fine-tuning process details are outlined, showcasing parameters and resources used. Model
predictions are saved.

### Predictions Evaluation

In this notebook, we assess the quality of code review predictions generated by the models. Both HuggingFace pre-trained and
fine-tuned models are evaluated across different datasets, shedding light on their performance.

Qualitative assessment is conducted to gain insights into how the models generate code reviews. We present samples of
code, along with predictions from both models, enabling a visual comparison with human reviews. This helps in
understanding the nuances of model-generated reviews.

Lastly, we quantitatively evaluate the models' performance using BLEU-4 scores. We calculate scores for each dataset,
providing a comprehensive overview of how well the models align with human reviews. This quantitative analysis helps in
drawing conclusions about the effectiveness of the models in code review prediction.

Throughout this journey, we aim to explore the capabilities and limitations of language models in the context of code
review, shedding light on their potential applications and areas for improvement.

## Table of Contents

```{tableofcontents}
```

## Bibliography

```{bibliography}
```
