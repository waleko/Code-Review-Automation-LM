{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Predictions Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc60e7255799cccf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.smooth_bleu import bleu_fromstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def analyze_preds(base_file, sample_size=5):\n",
    "    # read files\n",
    "    hf_preds_file = Path(base_file).with_suffix('.hf_pred.csv')\n",
    "    fine_tuned_file = Path(base_file).with_suffix('.finetuned_pred.csv')\n",
    "    hf_preds = pd.read_csv(hf_preds_file)\n",
    "    fine_tuned = pd.read_csv(fine_tuned_file)\n",
    "    # put in df\n",
    "    df = pd.DataFrame({'code': fine_tuned['code'], 'hf_pred': hf_preds['prediction'], 'fine_tuned_pred': fine_tuned['prediction']})\n",
    "    df.replace(np.nan, '', regex=True)\n",
    "    # print sample with predictions\n",
    "    sample = df.sample(sample_size)\n",
    "    for code, hf_pred, fine_tuned_pred in sample.to_numpy():\n",
    "        print('-------------------')\n",
    "        print(code)\n",
    "        print(f'HF Pred: {hf_pred}')\n",
    "        print(f'Fine Tuned Pred: {fine_tuned_pred}')\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76e1f5dc15fd5625"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_bleu(df):\n",
    "    refs = list(df['target'])\n",
    "    preds = list(df['prediction'])\n",
    "    for i in range(len(preds)):\n",
    "        chars = \"(_)`.\"\n",
    "        for c in chars:\n",
    "            preds[i] = preds[i].replace(c, \" \" + c + \" \")\n",
    "            preds[i] = \" \".join(preds[i].split())\n",
    "            refs[i] = refs[i].replace(c, \" \" + c + \" \")\n",
    "            refs[i] = \" \".join(refs[i].split())\n",
    "    return bleu_fromstr(preds, refs, rmstop=False)\n",
    "\n",
    "def calc_bleu_score(base_file):\n",
    "    hf_preds_file = Path(base_file).with_suffix('.hf_pred.csv')\n",
    "    fine_tuned_file = Path(base_file).with_suffix('.finetuned_pred.csv')\n",
    "    hf_preds = pd.read_csv(hf_preds_file)\n",
    "    ft_preds = pd.read_csv(fine_tuned_file)\n",
    "    hf_preds.replace(np.nan, '', regex=True, inplace=True)\n",
    "    ft_preds.replace(np.nan, '', regex=True, inplace=True)\n",
    "    hf_bleu = calc_bleu(hf_preds)\n",
    "    ft_bleu = calc_bleu(ft_preds)\n",
    "    print(f'HF BLEU: {hf_bleu}')\n",
    "    print(f'Fine Tuned BLEU: {ft_bleu}')\n",
    "    return hf_bleu, ft_bleu"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f5e6defa2df6726"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Qualitative Evaluation\n",
    "We will now compare the predictions of the HF model and the fine-tuned model on samples of the four datasets.\n",
    "\n",
    "We will print the code, the prediction of the HF model and the prediction of the fine-tuned model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3698532ffeff76b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = {}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "642eedcac45ee834"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['msg'] = analyze_preds('../data/msg-test')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30dc56606146a2d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['vscode'] = analyze_preds('../data/microsoft_vscode_1000.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19fbf302695a4c36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['kotlin'] = analyze_preds('../data/JetBrains_kotlin_1000.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6f7b55edbe73e06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['uppy'] = analyze_preds('../data/transloadit_uppy_1000.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3a70e469063ec64"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the fine-tuned model produces better predictions than the HF model. The predictions are much more insightful and detailed. The HF model tends to produce more generic predictions, while the fine-tuned model produces predictions that are more specific to the code."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88786c13e63fb984"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantitative Evaluation\n",
    "For each dataset, we calculate the [BLEU-4](https://en.wikipedia.org/wiki/BLEU) score for the predictions of the HF model and the fine-tuned model. The BLEU score is a measure of how similar the predictions are to the target. The higher the score, the better the predictions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c86e2ae168dbf9a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calc_bleu_score('../data/msg-test')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "860cd835ad7fc0c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calc_bleu_score('../data/microsoft_vscode_1000.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "baf7bf5989d87648"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calc_bleu_score('../data/JetBrains_kotlin_1000.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc3bea9368dc3041"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calc_bleu_score('../data/transloadit_uppy_1000.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2ed67a1d9adcf63"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the fine-tuned model performs better than the HF model on all datasets. The semantic value of the predictions is also better, as we can see in the qualitative evaluation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b14a888443bebefe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
