{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b65e7acd52a4668e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CodeReviewer Model Inference\n",
    "\n",
    "Let's generate code reviews using `microsoft/codereviewer` model {cite}`li2022codereviewer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a4776ea7be212fc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1 Tokenizers and Datasets\n",
    "\n",
    "P.S. Enormous thanks to the authors of {cite}`p4vv37_codebert_2023` for providing open-source for working with the tokenizer and the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad4d16d13804be69",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# download tokenizer from huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codereviewer\")\n",
    "\n",
    "# add required special tokens to the tokenizer\n",
    "tokenizer = utils.process_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb003d6d8f578da1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer):\n",
    "        self.y = df[\"human_review\"]\n",
    "        self.code = df[\"diff_hunk\"]\n",
    "        self.x = torch.tensor(df.apply(lambda row: utils.encode_diff(tokenizer, row[\"diff_hunk\"], '', ''), axis=1), dtype=torch.long).cpu()\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "   \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b01969568fe53c90",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2 Load data\n",
    "Here we load the data and create a dataloader for each project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d06f51b2150c61c4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = ['../data/msg-test.csv', '../data/JetBrains_kotlin_1000.csv', '../data/microsoft_vscode_1000.csv', '../data/transloadit_uppy_1000.csv']\n",
    "\n",
    "datasets = []\n",
    "dataloaders = []\n",
    "for filename in filenames:\n",
    "    df = pd.read_csv(filename)\n",
    "    dataset = ReviewsDataset(df, tokenizer)\n",
    "    datasets.append(dataset)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=False) # batch_size=6 for 8GB GPU\n",
    "    dataloaders.append(dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1381eaca0f99dfc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3 Predict\n",
    "\n",
    "Now we can generate code reviews for each project. We will use two models:\n",
    "- Pre-trained model from HuggingFace provided by the authors of {cite}`li2022codereviewer`\n",
    "- Fine-tuned model on the CodeReviewer dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef4c3e665f4be306",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a5b97449733bbc6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for X, y in tqdm(dataloader):\n",
    "        inputs_mask = X.ne(tokenizer.pad_id)\n",
    "        preds = model.generate(\n",
    "            X.to(device),\n",
    "            attention_mask=inputs_mask.to(device),\n",
    "            use_cache=True,\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "            max_length=512,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        preds_np = preds.detach().cpu().numpy()\n",
    "        preds_decoded = np.apply_along_axis(lambda row: tokenizer.decode(\n",
    "            row[2:], skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        ), 1, preds_np)\n",
    "        result += list(preds_decoded)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d84900c15fa4ffc3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### HuggingFace pre-trained checkpoint\n",
    "\n",
    "The model is available on the HuggingFace model hub: https://huggingface.co/microsoft/codereviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c508661efcdcad40",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 636/636 [10:03<00:00,  1.05it/s]\n",
      "100%|██████████| 63/63 [02:42<00:00,  2.58s/it]\n",
      "100%|██████████| 63/63 [01:46<00:00,  1.70s/it]\n",
      "100%|██████████| 63/63 [01:19<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# download the pretrained model from huggingface\n",
    "hf_model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/codereviewer\")\n",
    "\n",
    "for filename, dataset, dataloader in zip(filenames, datasets, dataloaders):\n",
    "    preds = predict(hf_model, dataloader)\n",
    "    df_pred = pd.DataFrame({'code': dataset.code, 'target': dataset.y, 'prediction': preds})\n",
    "    df_pred.to_csv(Path(filename).with_suffix('.hf_pred.csv'))\n",
    "    df_pred.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8e932357e193796",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Fine-tuned CodeReviewer\n",
    "\n",
    "I fine-tuned the model on the CodeReviewer dataset on the `msg` task using the [instructions](https://github.com/microsoft/CodeBERT/tree/master/CodeReviewer#3-finetuneinference) from the authors of {cite}`li2022codereviewer`.\n",
    "\n",
    "For the fine-tuning I used the following parameters:\n",
    "- `batch_size=16`\n",
    "- `learning_rate=3e-4`\n",
    "- `max_source_length=512`\n",
    "\n",
    "The execution took about 12 hours on a single NVIDIA GeForce A100 GPU. The model was fine-tuned for 3 epochs.\n",
    "\n",
    "I have made the checkpoint available on the HuggingFace model hub: https://huggingface.co/waleko/codereviewer-finetuned-msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "851255e54c49484a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 2.13k/2.13k [00:00<00:00, 2.06MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 892M/892M [00:22<00:00, 40.2MB/s] \n",
      "Some weights of the model checkpoint at waleko/codereviewer-finetuned-msg were not used when initializing T5ForConditionalGeneration: ['cls_head.weight', 'cls_head.bias']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)neration_config.json: 100%|██████████| 168/168 [00:00<00:00, 40.1kB/s]\n",
      "100%|██████████| 636/636 [14:22<00:00,  1.36s/it]\n",
      "100%|██████████| 63/63 [01:31<00:00,  1.45s/it]\n",
      "100%|██████████| 63/63 [01:24<00:00,  1.35s/it]\n",
      "100%|██████████| 63/63 [01:21<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# download the fine-tuned model\n",
    "ft_model = AutoModelForSeq2SeqLM.from_pretrained(\"waleko/codereviewer-finetuned-msg\")\n",
    "\n",
    "for filename, dataset, dataloader in zip(filenames, datasets, dataloaders):\n",
    "    preds = predict(ft_model, dataloader)\n",
    "    df_pred = pd.DataFrame({'code': dataset.code, 'target': dataset.y, 'prediction': preds})\n",
    "    df_pred.to_csv(Path(filename).with_suffix('.finetuned_pred.csv'))\n",
    "    df_pred.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
